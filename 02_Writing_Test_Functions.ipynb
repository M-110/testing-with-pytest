{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Writing_Test_Functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNavfHnhvCsM16BJzRjprhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-110/testing-with-pytest/blob/main/02_Writing_Test_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Highlights: \n",
        "\n",
        "* parameters"
      ],
      "metadata": {
        "id": "JlWluHCrwUrj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU3nnqrVeV1v"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSz4v-A_fvQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebfb7e8-2138-4554-e6b9-9a6924ffde56"
      },
      "source": [
        "%%writefile test_fail.py\n",
        "def test_eq():\n",
        "  assert 'Hello World' == 'Hello world'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_fail.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqcN6BVugZVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92d6e54-c446-41fd-ee42-ba1c253a2542"
      },
      "source": [
        "!pytest test_fail.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_fail.py::test_eq \u001b[31mFAILED\u001b[0m\u001b[36m                                             [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m___________________________________ test_eq ____________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_eq():\u001b[0m\n",
            "\u001b[1m>     assert 'Hello World' == 'Hello world'\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'Hello World' == 'Hello world'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       - Hello World\u001b[0m\n",
            "\u001b[1m\u001b[31mE       ?       ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + Hello world\u001b[0m\n",
            "\u001b[1m\u001b[31mE       ?       ^\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_fail.py\u001b[0m:2: AssertionError\n",
            "\u001b[1m\u001b[31m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EffSr0D9EAsS"
      },
      "source": [
        "# Exceptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t-IPjLsAz7_",
        "outputId": "c6c366fd-fde1-49af-b24e-2aa0a1735b62"
      },
      "source": [
        "%%writefile test_raise.py\n",
        "import pytest \n",
        "\n",
        "def test_raise():\n",
        "  with pytest.raises(ValueError):\n",
        "    raise ValueError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_raise.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXW4Tq0uBbdv",
        "outputId": "69d95902-809f-4b0f-e608-d6967d45b92b"
      },
      "source": [
        "!pytest test_raise.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_raise.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[36m                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WECukavOCZCS",
        "outputId": "ad1f5fb5-1823-4374-d9f3-0d0f8a86781b"
      },
      "source": [
        "%%writefile test_raise2.py\n",
        "import pytest \n",
        "\n",
        "def test_raise():\n",
        "  with pytest.raises(ValueError) as e:\n",
        "    raise ValueError('Oh no')\n",
        "  assert e.value.args[0] == 'Oh no'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_raise2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MPyiKpqD0zy",
        "outputId": "1594930c-d854-4d13-fbe9-03be34b2391a"
      },
      "source": [
        "!pytest test_raise2.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_raise2.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[36m                                        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yewXF_wfECoS"
      },
      "source": [
        "# Marking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKWynG91D2c3",
        "outputId": "31ee223a-7ffd-47c4-9cdc-0da433b3fc28"
      },
      "source": [
        "%%writefile test_fire.py\n",
        "import pytest\n",
        "\n",
        "\n",
        "def test_pool():\n",
        "  assert 'pool' == 'pool'\n",
        "\n",
        "@pytest.mark.smoke\n",
        "def test_tree():\n",
        "  assert 5 == 5\n",
        "\n",
        "@pytest.mark.kitchen\n",
        "@pytest.mark.smoke\n",
        "def test_stove():\n",
        "  assert 'stove' == 'stove'\n",
        "\n",
        "@pytest.mark.kitchen\n",
        "def test_microwave():\n",
        "  assert 0 == 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_fire.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQfnIAMEXLx",
        "outputId": "63905cdc-d6bb-4bf8-aff9-3b6be0c275ea"
      },
      "source": [
        "!pytest -v -m 'smoke'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items / 16 deselected                                             \u001b[0m\n",
            "\n",
            "test_fire.py::test_tree \u001b[32mPASSED\u001b[0m\u001b[36m                                           [ 50%]\u001b[0m\n",
            "test_fire.py::test_stove \u001b[32mPASSED\u001b[0m\u001b[36m                                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 2 passed, 16 deselected in 0.04 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTAlrml4EolL",
        "outputId": "bf7286f9-67cb-43ab-fdbc-365c4c3f5789"
      },
      "source": [
        "!pytest -v -m 'kitchen'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items / 16 deselected                                             \u001b[0m\n",
            "\n",
            "test_fire.py::test_stove \u001b[32mPASSED\u001b[0m\u001b[36m                                          [ 50%]\u001b[0m\n",
            "test_fire.py::test_microwave \u001b[32mPASSED\u001b[0m\u001b[36m                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 2 passed, 16 deselected in 0.02 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvErEAFyE0Yq",
        "outputId": "b44d42a7-a43c-4ea2-8342-85b26ee063ca"
      },
      "source": [
        "!pytest -v -m 'smoke and kitchen'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items / 17 deselected                                             \u001b[0m\n",
            "\n",
            "test_fire.py::test_stove \u001b[32mPASSED\u001b[0m\u001b[36m                                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 1 passed, 17 deselected in 0.02 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF6wleX4FIdA"
      },
      "source": [
        "# Skipping tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I9T9XybE3VK",
        "outputId": "d50657c8-240a-461a-9aec-002e4c5a5ba1"
      },
      "source": [
        "%%writefile test_skipping.py\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.skip(reason='misunderstood math')\n",
        "def test_math():\n",
        "  assert 2 + 2 == 5\n",
        "\n",
        "\n",
        "def test_math_2():\n",
        "  assert 2 + 2 == 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_skipping.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBtrSfn-Fra6",
        "outputId": "c98a3356-0e1f-4e76-c764-0e2e1bc9afbb"
      },
      "source": [
        "!pytest test_skipping.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_skipping.py::test_math \u001b[33mSKIPPED\u001b[0m\u001b[36m                                      [ 50%]\u001b[0m\n",
            "test_skipping.py::test_math_2 \u001b[32mPASSED\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m===================== 1 passed, 1 skipped in 0.02 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkvPpXLsF4Lj",
        "outputId": "b2cc66e8-2b18-4177-d32e-be1fafe0f91e"
      },
      "source": [
        "%%writefile test_skipif.py\n",
        "import pytest\n",
        "import sys\n",
        "\n",
        "@pytest.mark.skipif(sys.version[2] < '8',\n",
        "                    reason='Feature not supported until Python 3.8')\n",
        "def test_new_feature():\n",
        "  assert 5 + 5 == 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_skipif.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BbHJMUlFuwu",
        "outputId": "a91c6fcc-8816-4417-f7bb-62f9be29675d"
      },
      "source": [
        "!pytest test_skipif.py -v -r s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_skipif.py::test_new_feature \u001b[33mSKIPPED\u001b[0m\u001b[36m                                 [100%]\u001b[0m\n",
            "=========================== short test summary info ============================\n",
            "SKIP [1] test_skipif.py:4: Feature not supported until Python 3.8\n",
            "\n",
            "\u001b[1m\u001b[33m========================== 1 skipped in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-i1nf5PHOCU",
        "outputId": "ad5a2c9a-7774-43e4-d165-b55a998efea0"
      },
      "source": [
        "!pytest -v -rfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items                                                             \u001b[0m\n",
            "\n",
            "test_class.py::TestStuff::test_this \u001b[32mPASSED\u001b[0m\u001b[36m                               [  5%]\u001b[0m\n",
            "test_class.py::TestStuff::test_that \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 11%]\u001b[0m\n",
            "test_class.py::test_not_class \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 16%]\u001b[0m\n",
            "test_expected_fail.py::test_whatever \u001b[32mPASSED\u001b[0m\u001b[36m                              [ 22%]\u001b[0m\n",
            "test_expected_fail.py::test_fail_it \u001b[31mFAILED\u001b[0m\u001b[36m                               [ 27%]\u001b[0m\n",
            "test_expected_fail.py::test_new_feature \u001b[33mxfail\u001b[0m\u001b[36m                            [ 33%]\u001b[0m\n",
            "test_expected_fail.py::test_duck \u001b[33mxfail\u001b[0m\u001b[36m                                   [ 38%]\u001b[0m\n",
            "test_expected_fail.py::test_duck2 \u001b[31mFAILED\u001b[0m\u001b[36m                                 [ 44%]\u001b[0m\n",
            "test_fail.py::test_eq \u001b[31mFAILED\u001b[0m\u001b[36m                                             [ 50%]\u001b[0m\n",
            "test_fire.py::test_pool \u001b[32mPASSED\u001b[0m\u001b[36m                                           [ 55%]\u001b[0m\n",
            "test_fire.py::test_tree \u001b[32mPASSED\u001b[0m\u001b[36m                                           [ 61%]\u001b[0m\n",
            "test_fire.py::test_stove \u001b[32mPASSED\u001b[0m\u001b[36m                                          [ 66%]\u001b[0m\n",
            "test_fire.py::test_microwave \u001b[32mPASSED\u001b[0m\u001b[36m                                      [ 72%]\u001b[0m\n",
            "test_raise.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[36m                                         [ 77%]\u001b[0m\n",
            "test_raise2.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[36m                                        [ 83%]\u001b[0m\n",
            "test_skipif.py::test_new_feature \u001b[33mSKIPPED\u001b[0m\u001b[36m                                 [ 88%]\u001b[0m\n",
            "test_skipping.py::test_math \u001b[33mSKIPPED\u001b[0m\u001b[36m                                      [ 94%]\u001b[0m\n",
            "test_skipping.py::test_math_2 \u001b[32mPASSED\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________________ test_fail_it _________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_fail_it():\u001b[0m\n",
            "\u001b[1m>     assert 5 == 4\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 4\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_expected_fail.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m__________________________________ test_duck2 __________________________________\u001b[0m\n",
            "[XPASS(strict)] \n",
            "\u001b[1m\u001b[31m___________________________________ test_eq ____________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_eq():\u001b[0m\n",
            "\u001b[1m>     assert 'Hello World' == 'Hello world'\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'Hello World' == 'Hello world'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       - Hello World\u001b[0m\n",
            "\u001b[1m\u001b[31mE       ?       ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + Hello world\u001b[0m\n",
            "\u001b[1m\u001b[31mE       ?       ^\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_fail.py\u001b[0m:2: AssertionError\n",
            "=========================== short test summary info ============================\n",
            "FAIL test_expected_fail.py::test_fail_it\n",
            "FAIL test_expected_fail.py::test_duck2\n",
            "FAIL test_fail.py::test_eq\n",
            "SKIP [1] test_skipif.py:4: Feature not supported until Python 3.8\n",
            "SKIP [1] test_skipping.py:3: misunderstood math\n",
            "\u001b[1m\u001b[31m========== 3 failed, 11 passed, 2 skipped, 2 xfailed in 0.07 seconds ===========\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx20GL7tH1si"
      },
      "source": [
        "# Marking Tests as Expecting to Fail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmu3z4VMH345",
        "outputId": "7f64cf15-76d9-4bc1-f4c0-fb9d7a0c2391"
      },
      "source": [
        "%%writefile test_expected_fail.py\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "\n",
        "def test_whatever():\n",
        "  assert 5 == 5\n",
        "\n",
        "def test_fail_it():\n",
        "  assert 5 == 4\n",
        "\n",
        "@pytest.mark.xfail(sys.version[2] < '8',\n",
        "                    reason='Feature not supported until Python 3.8')\n",
        "def test_new_feature():\n",
        "  assert 5 == 0\n",
        "\n",
        "@pytest.mark.xfail()\n",
        "def test_duck():\n",
        "  assert 'duck' == 'human'\n",
        "\n",
        "@pytest.mark.xfail()\n",
        "def test_duck2():\n",
        "  assert 'duck'== 'duck'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_expected_fail.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cgMoeElIhzl",
        "outputId": "6c167b0c-a6d9-4a99-d3a4-c6fdcc264687"
      },
      "source": [
        "!pytest test_expected_fail.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_expected_fail.py::test_whatever \u001b[32mPASSED\u001b[0m\u001b[36m                              [ 20%]\u001b[0m\n",
            "test_expected_fail.py::test_fail_it \u001b[31mFAILED\u001b[0m\u001b[36m                               [ 40%]\u001b[0m\n",
            "test_expected_fail.py::test_new_feature \u001b[33mxfail\u001b[0m\u001b[36m                            [ 60%]\u001b[0m\n",
            "test_expected_fail.py::test_duck \u001b[33mxfail\u001b[0m\u001b[36m                                   [ 80%]\u001b[0m\n",
            "test_expected_fail.py::test_duck2 \u001b[31mFAILED\u001b[0m\u001b[36m                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________________ test_fail_it _________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_fail_it():\u001b[0m\n",
            "\u001b[1m>     assert 5 == 4\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 4\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_expected_fail.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m__________________________________ test_duck2 __________________________________\u001b[0m\n",
            "[XPASS(strict)] \n",
            "\u001b[1m\u001b[31m================ 2 failed, 1 passed, 2 xfailed in 0.04 seconds =================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRNSwn1YIlLo",
        "outputId": "972fe650-1f3b-406f-f721-0f127d444893"
      },
      "source": [
        "%%writefile pytest.ini\n",
        "[pytest]\n",
        "xfail_strict=true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting pytest.ini\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjxpSsTdJIE6",
        "outputId": "f51d37e4-9fac-4190-e631-ee06759f3da0"
      },
      "source": [
        "# Now XPASS (expected to fail but passed) will be marked as FAILED\n",
        "!pytest test_expected_fail.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_expected_fail.py::test_whatever \u001b[32mPASSED\u001b[0m\u001b[36m                              [ 20%]\u001b[0m\n",
            "test_expected_fail.py::test_fail_it \u001b[31mFAILED\u001b[0m\u001b[36m                               [ 40%]\u001b[0m\n",
            "test_expected_fail.py::test_new_feature \u001b[33mxfail\u001b[0m\u001b[36m                            [ 60%]\u001b[0m\n",
            "test_expected_fail.py::test_duck \u001b[33mxfail\u001b[0m\u001b[36m                                   [ 80%]\u001b[0m\n",
            "test_expected_fail.py::test_duck2 \u001b[31mFAILED\u001b[0m\u001b[36m                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________________ test_fail_it _________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_fail_it():\u001b[0m\n",
            "\u001b[1m>     assert 5 == 4\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 4\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_expected_fail.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m__________________________________ test_duck2 __________________________________\u001b[0m\n",
            "[XPASS(strict)] \n",
            "\u001b[1m\u001b[31m================ 2 failed, 1 passed, 2 xfailed in 0.03 seconds =================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xcDpe99JZOb"
      },
      "source": [
        "# Subset of Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNgtVGdKJa1n",
        "outputId": "067a0b46-dab8-4cea-a6fd-87ca187ea07c"
      },
      "source": [
        "!pytest --tb=no"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items                                                             \u001b[0m\n",
            "\n",
            "test_class.py ...\u001b[36m                                                        [ 16%]\u001b[0m\n",
            "test_expected_fail.py .FxxF\u001b[36m                                              [ 44%]\u001b[0m\n",
            "test_fail.py F\u001b[36m                                                           [ 50%]\u001b[0m\n",
            "test_fire.py ....\u001b[36m                                                        [ 72%]\u001b[0m\n",
            "test_raise.py .\u001b[36m                                                          [ 77%]\u001b[0m\n",
            "test_raise2.py .\u001b[36m                                                         [ 83%]\u001b[0m\n",
            "test_skipif.py s\u001b[36m                                                         [ 88%]\u001b[0m\n",
            "test_skipping.py s.\u001b[36m                                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m========== 3 failed, 11 passed, 2 skipped, 2 xfailed in 0.06 seconds ===========\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysT5DS26KEI4"
      },
      "source": [
        "Test file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hohHX4lCJI3i",
        "outputId": "479f1680-2a36-403c-d337-dba9fd423ef9"
      },
      "source": [
        "!pytest test_fire.py --tb=no"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_fire.py ....\u001b[36m                                                        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 4 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOvH3er7KFVw"
      },
      "source": [
        "Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNIN1T9mJ1w_",
        "outputId": "14465c83-47f2-4356-bacd-603dcd9b3cec"
      },
      "source": [
        "!pytest test_fire.py::test_pool"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_fire.py .\u001b[36m                                                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6SbhVJ9KIPN"
      },
      "source": [
        "Test class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxCQT91wKAPN",
        "outputId": "c0604ee0-b726-4da6-c452-63c4488ab8ef"
      },
      "source": [
        "%%writefile test_class.py\n",
        "\n",
        "class TestStuff:\n",
        "  def test_this(self):\n",
        "    assert True == True\n",
        "  \n",
        "  def test_that(self):\n",
        "    assert True == True\n",
        "\n",
        "def test_not_class():\n",
        "  assert False == False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_class.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9biTVE3tKUdj",
        "outputId": "9521da0e-73bf-4751-835d-d7f5df04ada7"
      },
      "source": [
        "!pytest test_class.py::TestStuff -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_class.py::TestStuff::test_this \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 50%]\u001b[0m\n",
            "test_class.py::TestStuff::test_that \u001b[32mPASSED\u001b[0m\u001b[36m                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 2 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZP1qu6AKjbl"
      },
      "source": [
        "Test class method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-UrQR-iKWRw",
        "outputId": "f5f73c59-afe9-4732-d29d-95282a31f52d"
      },
      "source": [
        "!pytest test_class.py::TestStuff::test_this -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_class.py::TestStuff::test_this \u001b[32mPASSED\u001b[0m\u001b[36m                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3B6fHgsK3QU"
      },
      "source": [
        "Test based on test name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj-J2OqfKnpV",
        "outputId": "9667ca4f-0ff7-497d-c77e-ccbce968feca"
      },
      "source": [
        "!pytest -v -k \"duck\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items / 16 deselected                                             \u001b[0m\n",
            "\n",
            "test_expected_fail.py::test_duck \u001b[33mxfail\u001b[0m\u001b[36m                                   [ 50%]\u001b[0m\n",
            "test_expected_fail.py::test_duck2 \u001b[31mFAILED\u001b[0m\u001b[36m                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m__________________________________ test_duck2 __________________________________\u001b[0m\n",
            "[XPASS(strict)] \n",
            "\u001b[1m\u001b[31m============== 1 failed, 16 deselected, 1 xfailed in 0.04 seconds ==============\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7UGSk61K_Nw",
        "outputId": "e24aad24-2ef2-430d-f57d-90b248dab907"
      },
      "source": [
        "!pytest -v -k \"duck or math\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 13 items                                                            \u001b[0m\u001b[1m\rcollecting 14 items                                                            \u001b[0m\u001b[1m\rcollecting 15 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 18 items                                                            \u001b[0m\u001b[1m\rcollected 18 items / 14 deselected                                             \u001b[0m\n",
            "\n",
            "test_expected_fail.py::test_duck \u001b[33mxfail\u001b[0m\u001b[36m                                   [ 25%]\u001b[0m\n",
            "test_expected_fail.py::test_duck2 \u001b[31mFAILED\u001b[0m\u001b[36m                                 [ 50%]\u001b[0m\n",
            "test_skipping.py::test_math \u001b[33mSKIPPED\u001b[0m\u001b[36m                                      [ 75%]\u001b[0m\n",
            "test_skipping.py::test_math_2 \u001b[32mPASSED\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m__________________________________ test_duck2 __________________________________\u001b[0m\n",
            "[XPASS(strict)] \n",
            "\u001b[1m\u001b[31m=== 1 failed, 1 passed, 1 skipped, 14 deselected, 1 xfailed in 0.04 seconds ====\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzo4clvtLM_d"
      },
      "source": [
        "# Parametrize Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51j3oQ0uLJIv",
        "outputId": "30d55ba9-4926-4c4e-84b2-779d1c5db1a9"
      },
      "source": [
        "%%writefile test_params.py\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize('number',\n",
        "                         [5,\n",
        "                          6,\n",
        "                          7,\n",
        "                          8])\n",
        "def test_number_lt(number):\n",
        "  assert number < 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_params.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2DtFlOPc4yP",
        "outputId": "e1091849-760c-48ed-db61-5cec15b2dd90"
      },
      "source": [
        "!pytest test_params.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_params.py::test_number_lt[5] \u001b[32mPASSED\u001b[0m\u001b[36m                                 [ 25%]\u001b[0m\n",
            "test_params.py::test_number_lt[6] \u001b[32mPASSED\u001b[0m\u001b[36m                                 [ 50%]\u001b[0m\n",
            "test_params.py::test_number_lt[7] \u001b[32mPASSED\u001b[0m\u001b[36m                                 [ 75%]\u001b[0m\n",
            "test_params.py::test_number_lt[8] \u001b[31mFAILED\u001b[0m\u001b[36m                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m______________________________ test_number_lt[8] _______________________________\u001b[0m\n",
            "\n",
            "number = 8\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize('number',\u001b[0m\n",
            "\u001b[1m                             [5,\u001b[0m\n",
            "\u001b[1m                              6,\u001b[0m\n",
            "\u001b[1m                              7,\u001b[0m\n",
            "\u001b[1m                              8])\u001b[0m\n",
            "\u001b[1m    def test_number_lt(number):\u001b[0m\n",
            "\u001b[1m>     assert number < 8\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 8 < 8\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_params.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m====================== 1 failed, 3 passed in 0.03 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxfunnejc5yI",
        "outputId": "a1922337-72c2-4d15-d1a9-0e40f30ecef4"
      },
      "source": [
        "%%writefile test_params2.py\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize('x, y, expected',\n",
        "                         [(1,1,2),\n",
        "                          (2,2,4),\n",
        "                          (3,3,5),\n",
        "                          (4,4,8)])\n",
        "def test_number_lt(x, y, expected):\n",
        "  assert x + y == expected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_params2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulj75N7ndUX3",
        "outputId": "07c5d37e-06dc-487e-b8c0-d915e3dc06ce"
      },
      "source": [
        "!pytest test_params2.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_params2.py::test_number_lt[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 25%]\u001b[0m\n",
            "test_params2.py::test_number_lt[2-2-4] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 50%]\u001b[0m\n",
            "test_params2.py::test_number_lt[3-3-5] \u001b[31mFAILED\u001b[0m\u001b[36m                            [ 75%]\u001b[0m\n",
            "test_params2.py::test_number_lt[4-4-8] \u001b[32mPASSED\u001b[0m\u001b[36m                            [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m____________________________ test_number_lt[3-3-5] _____________________________\u001b[0m\n",
            "\n",
            "x = 3, y = 3, expected = 5\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize('x, y, expected',\u001b[0m\n",
            "\u001b[1m                             [(1,1,2),\u001b[0m\n",
            "\u001b[1m                              (2,2,4),\u001b[0m\n",
            "\u001b[1m                              (3,3,5),\u001b[0m\n",
            "\u001b[1m                              (4,4,8)])\u001b[0m\n",
            "\u001b[1m    def test_number_lt(x, y, expected):\u001b[0m\n",
            "\u001b[1m>     assert x + y == expected\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert (3 + 3) == 5\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_params2.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m====================== 1 failed, 3 passed in 0.04 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyzZTkNNdtNT"
      },
      "source": [
        "Order doesn't matter, parameters are passed as keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgaccc4ddVV6",
        "outputId": "cbc10537-2686-4227-8498-54772954f140"
      },
      "source": [
        "%%writefile test_params3.py\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize('expected, y, x',\n",
        "                         [(2,1,1),\n",
        "                          (4,2,2),\n",
        "                          (6,3,3),\n",
        "                          (9,4,4)])\n",
        "def test_number_lt(x, y, expected):\n",
        "  assert x + y == expected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_params3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vbt-gsldg5L",
        "outputId": "a3ee8c62-fb1a-4a87-98b1-003a25f1ab79"
      },
      "source": [
        "!pytest test_params3.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_params3.py::test_number_lt[2-1-1] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 25%]\u001b[0m\n",
            "test_params3.py::test_number_lt[4-2-2] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 50%]\u001b[0m\n",
            "test_params3.py::test_number_lt[6-3-3] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 75%]\u001b[0m\n",
            "test_params3.py::test_number_lt[9-4-4] \u001b[31mFAILED\u001b[0m\u001b[36m                            [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m____________________________ test_number_lt[9-4-4] _____________________________\u001b[0m\n",
            "\n",
            "x = 4, y = 4, expected = 9\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize('expected, y, x',\u001b[0m\n",
            "\u001b[1m                             [(2,1,1),\u001b[0m\n",
            "\u001b[1m                              (4,2,2),\u001b[0m\n",
            "\u001b[1m                              (6,3,3),\u001b[0m\n",
            "\u001b[1m                              (9,4,4)])\u001b[0m\n",
            "\u001b[1m    def test_number_lt(x, y, expected):\u001b[0m\n",
            "\u001b[1m>     assert x + y == expected\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert (4 + 4) == 9\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_params3.py\u001b[0m:9: AssertionError\n",
            "\u001b[1m\u001b[31m====================== 1 failed, 3 passed in 0.04 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdUBzU5teJZt"
      },
      "source": [
        "You can even specify a specific parameter. But it must be one that is already in the list of parameters to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obB0XHyedhuR",
        "outputId": "5af0eccb-aa48-4cc9-a182-791a4efa218e"
      },
      "source": [
        "!pytest test_params3.py::test_number_lt[6-3-3] -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_params3.py::test_number_lt[6-3-3] \u001b[32mPASSED\u001b[0m\u001b[36m                            [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX6KA249fGlY"
      },
      "source": [
        "Use the same test arguments in multiple tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqy0tnnpeDiX",
        "outputId": "2d0200c4-e878-4223-bb30-cbe8a607b08d"
      },
      "source": [
        "%%writefile test_params4.py\n",
        "import pytest\n",
        "\n",
        "numbers_to_try = [(1,1),\n",
        "                  (2,3),\n",
        "                  (-1,5),\n",
        "                  (0,8)]\n",
        "\n",
        "@pytest.mark.parametrize('a, b', numbers_to_try)\n",
        "def test_number_add(a, b):\n",
        "  assert a + b == b + a\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('a, b', numbers_to_try)\n",
        "def test_number_mul(a, b):\n",
        "  assert a * b == b * a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_params4.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPrSGQ0Ce6xK",
        "outputId": "7939103c-babc-42c8-8652-69a7eef935e7"
      },
      "source": [
        "!pytest test_params4.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 8 items                                                             \u001b[0m\u001b[1m\rcollected 8 items                                                              \u001b[0m\n",
            "\n",
            "test_params4.py::test_number_add[1-1] \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 12%]\u001b[0m\n",
            "test_params4.py::test_number_add[2-3] \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 25%]\u001b[0m\n",
            "test_params4.py::test_number_add[-1-5] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 37%]\u001b[0m\n",
            "test_params4.py::test_number_add[0-8] \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 50%]\u001b[0m\n",
            "test_params4.py::test_number_mul[1-1] \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 62%]\u001b[0m\n",
            "test_params4.py::test_number_mul[2-3] \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 75%]\u001b[0m\n",
            "test_params4.py::test_number_mul[-1-5] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 87%]\u001b[0m\n",
            "test_params4.py::test_number_mul[0-8] \u001b[32mPASSED\u001b[0m\u001b[36m                             [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 8 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l3BpEHsfZhu"
      },
      "source": [
        "Param ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMwQ2nfPe89h",
        "outputId": "1be427f8-8c8f-4ccb-ed6b-d0fe64b69189"
      },
      "source": [
        "%%writefile test_params5.py\n",
        "import pytest\n",
        "\n",
        "class Cat:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "cats = [Cat('pete'),\n",
        "        Cat('felix'),\n",
        "        Cat('sylvester')]\n",
        "\n",
        "@pytest.mark.parametrize('cat', cats)\n",
        "def test_cat(cat):\n",
        "  assert cat == cat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_params5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t6XuTwEgMc4"
      },
      "source": [
        "Not great names 'cat0', 'cat1', 'cat2'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq7ZOxdpgF8T",
        "outputId": "19157d5c-fdc8-4adc-bca2-2f2510d8bc2d"
      },
      "source": [
        "!pytest test_params5.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_params5.py::test_cat[cat0] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [ 33%]\u001b[0m\n",
            "test_params5.py::test_cat[cat1] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [ 66%]\u001b[0m\n",
            "test_params5.py::test_cat[cat2] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 3 passed in 0.04 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiOhDNDxgHV0",
        "outputId": "3f2a5172-6b34-49db-a06c-ba88a7ade34a"
      },
      "source": [
        "%%writefile test_params6.py\n",
        "import pytest\n",
        "\n",
        "class Cat:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "cats = [Cat('pete'),\n",
        "        Cat('felix'),\n",
        "        Cat('sylvester')]\n",
        "\n",
        "cat_ids = [cat.name for cat in cats]\n",
        "\n",
        "@pytest.mark.parametrize('cat', cats, ids=cat_ids)\n",
        "def test_cat(cat):\n",
        "  assert cat == cat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_params6.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVTOnxTXgYnF"
      },
      "source": [
        "Much better 😀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1FiXJ9qgVIx",
        "outputId": "b27287a4-d17f-48ed-d761-ad3226402fc5"
      },
      "source": [
        "!pytest test_params6.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_params6.py::test_cat[pete] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [ 33%]\u001b[0m\n",
            "test_params6.py::test_cat[felix] \u001b[32mPASSED\u001b[0m\u001b[36m                                  [ 66%]\u001b[0m\n",
            "test_params6.py::test_cat[sylvester] \u001b[32mPASSED\u001b[0m\u001b[36m                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 3 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPiHKMtgyVO"
      },
      "source": [
        "Parametrize classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt3qhMOtgWWH",
        "outputId": "7f7154bd-4978-4dd8-d318-0ce5ce41884b"
      },
      "source": [
        "%%writefile test_params7.py\n",
        "import pytest\n",
        "\n",
        "class Cat:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "cats = [Cat('pete'),\n",
        "        Cat('felix'),\n",
        "        Cat('sylvester')]\n",
        "\n",
        "cat_ids = [cat.name for cat in cats]\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('cat', cats, ids=cat_ids)\n",
        "class TestCats:\n",
        "  def test_cat_exists(self, cat):\n",
        "    assert cat is not None\n",
        "    \n",
        "  def test_cat_is_not_a_duck(self, cat):\n",
        "    assert cat != 'duck'\n",
        "\n",
        "  def test_cat_has_a_name(self, cat):\n",
        "    assert cat.name is not None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_params7.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPxw2jgzhMh6",
        "outputId": "99a54b09-0541-48d5-eb45-c84311f533a6"
      },
      "source": [
        "!pytest test_params7.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollecting 9 items                                                             \u001b[0m\u001b[1m\rcollected 9 items                                                              \u001b[0m\n",
            "\n",
            "test_params7.py::TestCats::test_cat_exists[pete] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 11%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_exists[felix] \u001b[32mPASSED\u001b[0m\u001b[36m                 [ 22%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_exists[sylvester] \u001b[32mPASSED\u001b[0m\u001b[36m             [ 33%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_is_not_a_duck[pete] \u001b[32mPASSED\u001b[0m\u001b[36m           [ 44%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_is_not_a_duck[felix] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 55%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_is_not_a_duck[sylvester] \u001b[32mPASSED\u001b[0m\u001b[36m      [ 66%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_has_a_name[pete] \u001b[32mPASSED\u001b[0m\u001b[36m              [ 77%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_has_a_name[felix] \u001b[32mPASSED\u001b[0m\u001b[36m             [ 88%]\u001b[0m\n",
            "test_params7.py::TestCats::test_cat_has_a_name[sylvester] \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 9 passed in 0.04 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BN0eT8qheyu"
      },
      "source": [
        "Assign individual ids to each param"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unW6guHKhPhB",
        "outputId": "a873ce01-e5fd-4c59-a76f-b9344d73bba7"
      },
      "source": [
        "%%writefile test_params8.py\n",
        "import pytest\n",
        "\n",
        "class Cat:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('cat',\n",
        "                         [pytest.param(Cat('pete'), id='cat with a name'),\n",
        "                          pytest.param(Cat(None), id='a cat with no name'),\n",
        "                          pytest.param(Cat(5), id='cat with a number for a name')])\n",
        "def test_cat_existence(cat):\n",
        "  assert cat is not None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_params8.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJwNXsJ_iORP",
        "outputId": "ce6497e8-8892-43eb-f05b-97cb1a8613f5"
      },
      "source": [
        "!pytest test_params8.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile: pytest.ini\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_params8.py::test_cat_existence[cat with a name] \u001b[32mPASSED\u001b[0m\u001b[36m              [ 33%]\u001b[0m\n",
            "test_params8.py::test_cat_existence[a cat with no name] \u001b[32mPASSED\u001b[0m\u001b[36m           [ 66%]\u001b[0m\n",
            "test_params8.py::test_cat_existence[cat with a number for a name] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 3 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}