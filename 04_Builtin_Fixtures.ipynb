{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Builtin_Fixtures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYI6AHOrAmQaQjqmNLKWvh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-110/testing-with-pytest/blob/main/04_Builtin_Fixtures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Highlights\n",
        "\n",
        "* tmpdir\n",
        "* create file as a fixture\n",
        "* add options to parser\n",
        "* cache\n",
        "* capsys capture stdout/errors\n",
        "* doc test tests"
      ],
      "metadata": {
        "id": "tOa-N2cZ7C0S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGHwv1O2ek0p"
      },
      "source": [
        "# Using tmpdir and tmpdir_factory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAvYPL0Beosg",
        "outputId": "89eb2b46-c632-42cb-ea75-24ffe6d4df71"
      },
      "source": [
        "%%writefile test_tmpdir.py\n",
        "import pytest\n",
        "\n",
        "\n",
        "def test_tmpdir(tmpdir):\n",
        "  my_file = tmpdir.join('something.txt')\n",
        "  my_dir = tmpdir.mkdir('some_dir')\n",
        "  another_file = my_dir.join('something_else.txt')\n",
        "  \n",
        "  my_file.write('this is my first file')\n",
        "  \n",
        "  another_file.write('this is my second file.')\n",
        "  \n",
        "  assert 'first' in my_file.read()\n",
        "  assert 'first' in another_file.read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_tmpdir.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ3mcahvfu3r",
        "outputId": "34b5b0ec-36e0-49ab-96f4-dbca77588951"
      },
      "source": [
        "!pytest test_tmpdir.py --setup-show -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_tmpdir.py::test_tmpdir \n",
            "SETUP    S doctest_namespace\n",
            "SETUP    S tmpdir_factory\n",
            "      SETUP    F add_mm (fixtures used: doctest_namespace)\n",
            "      SETUP    F tmpdir (fixtures used: tmpdir_factory)\n",
            "        test_tmpdir.py::test_tmpdir (fixtures used: add_mm, doctest_namespace, tmpdir, tmpdir_factory)\u001b[31mFAILED\u001b[0m\n",
            "      TEARDOWN F tmpdir\n",
            "      TEARDOWN F add_mm\n",
            "TEARDOWN S tmpdir_factory\n",
            "TEARDOWN S doctest_namespace\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________________ test_tmpdir __________________________________\u001b[0m\n",
            "\n",
            "tmpdir = local('/tmp/pytest-of-root/pytest-0/test_tmpdir0')\n",
            "\n",
            "\u001b[1m    def test_tmpdir(tmpdir):\u001b[0m\n",
            "\u001b[1m      my_file = tmpdir.join('something.txt')\u001b[0m\n",
            "\u001b[1m      my_dir = tmpdir.mkdir('some_dir')\u001b[0m\n",
            "\u001b[1m      another_file = my_dir.join('something_else.txt')\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      my_file.write('this is my first file')\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      another_file.write('this is my second file.')\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      assert 'first' in my_file.read()\u001b[0m\n",
            "\u001b[1m>     assert 'first' in another_file.read()\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'first' in 'this is my second file.'\u001b[0m\n",
            "\u001b[1m\u001b[31mE      +  where 'this is my second file.' = <bound method LocalPath.read of local('/tmp/pytest-of-root/pytest-0/test_tmpdir0/some_dir/something_else.txt')>()\u001b[0m\n",
            "\u001b[1m\u001b[31mE      +    where <bound method LocalPath.read of local('/tmp/pytest-of-root/pytest-0/test_tmpdir0/some_dir/something_else.txt')> = local('/tmp/pytest-of-root/pytest-0/test_tmpdir0/some_dir/something_else.txt').read\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_tmpdir.py\u001b[0m:14: AssertionError\n",
            "\u001b[1m\u001b[31m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K05H3VPpgDlK",
        "outputId": "88da19d2-70bc-45ca-9617-e5c3528afd19"
      },
      "source": [
        "%%writefile test_tmpdir2.py\n",
        "import pytest\n",
        "\n",
        "def test_tmpdir_factory(tmpdir_factory):\n",
        "  a_dir = tmpdir_factory.mktemp('mydir')\n",
        "  base_temp = tmpdir_factory.getbasetemp()\n",
        "  print('base:', base_temp)\n",
        "  my_file = a_dir.join('something.txt')\n",
        "  my_dir = a_dir.mkdir('some_dir')\n",
        "  another_file = my_dir.join('something_else.txt')\n",
        "  \n",
        "  my_file.write('this is my first file')\n",
        "  \n",
        "  another_file.write('this is my second file.')\n",
        "  \n",
        "  assert 'first' in my_file.read()\n",
        "  assert 'second' in another_file.read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_tmpdir2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSR0GSgpgFR1",
        "outputId": "2db1b871-c003-4ef9-c908-b2a4b41e2d20"
      },
      "source": [
        "!pytest test_tmpdir2.py -s --setup-show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_tmpdir2.py \n",
            "SETUP    S tmpdir_factory\n",
            "SETUP    S doctest_namespace\n",
            "      SETUP    F add_mm (fixtures used: doctest_namespace)\n",
            "        test_tmpdir2.py::test_tmpdir_factory (fixtures used: add_mm, doctest_namespace, tmpdir_factory)('base:', local('/tmp/pytest-of-root/pytest-1'))\n",
            ".\n",
            "      TEARDOWN F add_mm\n",
            "TEARDOWN S doctest_namespace\n",
            "TEARDOWN S tmpdir_factory\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aif6McVgybp",
        "outputId": "57312292-5bb4-4e52-c6de-0c94d945623e"
      },
      "source": [
        "%%writefile conftest.py\n",
        "import json\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture(scope='module')\n",
        "def author_file_json(tmpdir_factory):\n",
        "  python_author_data = dict(Ned=dict(City='Boston'),\n",
        "                            Brian=dict(City='Portland'),\n",
        "                            Luciano=dict(City='Sao Paulo'))\n",
        "  file = tmpdir_factory.mktemp('data').join('author_file.json')\n",
        "  print('file: ', str(file))\n",
        "\n",
        "  with file.open('w') as f:\n",
        "    json.dump(python_author_data, f)\n",
        "  return file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n3Hu-EiieOj",
        "outputId": "bdbd6e5f-2736-491f-8cab-68d9ec331a17"
      },
      "source": [
        "%%writefile test_authors.py\n",
        "import json\n",
        "\n",
        "def test_brian(author_file_json):\n",
        "  with author_file_json.open() as f:\n",
        "    authors = json.load(f)\n",
        "  assert authors['Brian']['City'] == 'Portland'\n",
        "\n",
        "def test_all_have_cities(author_file_json):\n",
        "  with author_file_json.open() as f:\n",
        "    authors = json.load(f)\n",
        "  for a in authors:\n",
        "    assert len(authors[a]['City']) > 0\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_authors.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afDujbNni6V4",
        "outputId": "f4807734-3a14-410e-c169-eb4d43eff6d8"
      },
      "source": [
        "!pytest test_authors.py --setup-show -s -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_authors.py::test_brian \n",
            "SETUP    S tmpdir_factory('file: ', '/tmp/pytest-of-root/pytest-2/data0/author_file.json')\n",
            "\n",
            "  SETUP    M author_file_json (fixtures used: tmpdir_factory)\n",
            "        test_authors.py::test_brian (fixtures used: author_file_json, tmpdir_factory)\u001b[32mPASSED\u001b[0m\n",
            "test_authors.py::test_all_have_cities \n",
            "        test_authors.py::test_all_have_cities (fixtures used: author_file_json, tmpdir_factory)\u001b[32mPASSED\u001b[0m\n",
            "  TEARDOWN M author_file_json\n",
            "TEARDOWN S tmpdir_factory\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 2 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XtVvfiUXCD9"
      },
      "source": [
        "# Using pytest config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSI9lcH-XF8I",
        "outputId": "78414e51-317c-4d6f-f510-ea7da126f9e3"
      },
      "source": [
        "%%writefile conftest.py\n",
        "def pytest_addoption(parser):\n",
        "  parser.addoption('--myopt', action='store_true', help='some bool option')\n",
        "  parser.addoption('--pet', action='store', default='cat', help='pet: cat or dog')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3c6g4MYi9Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190c517e-49bf-47a9-d885-5b3e91bcbd27"
      },
      "source": [
        "!pytest --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "\n",
            "positional arguments:\n",
            "  file_or_dir\n",
            "\n",
            "general:\n",
            "  -k EXPRESSION         only run tests which match the given substring\n",
            "                        expression. An expression is a python evaluatable\n",
            "                        expression where all names are substring-matched\n",
            "                        against test names and their parent classes. Example:\n",
            "                        -k 'test_method or test_other' matches all test\n",
            "                        functions and classes whose name contains\n",
            "                        'test_method' or 'test_other', while -k 'not\n",
            "                        test_method' matches those that don't contain\n",
            "                        'test_method' in their names. Additionally keywords\n",
            "                        are matched to classes and functions containing extra\n",
            "                        names in their 'extra_keyword_matches' set, as well as\n",
            "                        functions which have names assigned directly to them.\n",
            "  -m MARKEXPR           only run tests matching given mark expression.\n",
            "                        example: -m 'mark1 and not mark2'.\n",
            "  --markers             show markers (builtin, plugin and per-project ones).\n",
            "  -x, --exitfirst       exit instantly on first error or failed test.\n",
            "  --maxfail=num         exit after first num failures or errors.\n",
            "  --strict              marks not registered in configuration file raise\n",
            "                        errors.\n",
            "  -c file               load configuration from `file` instead of trying to\n",
            "                        locate one of the implicit configuration files.\n",
            "  --continue-on-collection-errors\n",
            "                        Force test execution even if collection errors occur.\n",
            "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
            "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
            "                        absolute path: '/home/user/root_dir'; path with\n",
            "                        variables: '$HOME/root_dir'.\n",
            "  --fixtures, --funcargs\n",
            "                        show available fixtures, sorted by plugin appearance\n",
            "                        (fixtures with leading '_' are only shown with '-v')\n",
            "  --fixtures-per-test   show fixtures per test\n",
            "  --import-mode={prepend,append}\n",
            "                        prepend/append to sys.path when importing test\n",
            "                        modules, default is to prepend.\n",
            "  --pdb                 start the interactive Python debugger on errors or\n",
            "                        KeyboardInterrupt.\n",
            "  --pdbcls=modulename:classname\n",
            "                        start a custom interactive Python debugger on errors.\n",
            "                        For example:\n",
            "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
            "  --capture=method      per-test capturing method: one of fd|sys|no.\n",
            "  -s                    shortcut for --capture=no.\n",
            "  --runxfail            run tests even if they are marked xfail\n",
            "  --lf, --last-failed   rerun only the tests that failed at the last run (or\n",
            "                        all if none failed)\n",
            "  --ff, --failed-first  run all tests but run the last failures first. This\n",
            "                        may re-order tests and thus lead to repeated fixture\n",
            "                        setup/teardown\n",
            "  --nf, --new-first     run tests from new files first, then the rest of the\n",
            "                        tests sorted by file mtime\n",
            "  --cache-show          show cache contents, don't perform collection or tests\n",
            "  --cache-clear         remove all cache contents at start of test run.\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        change the behavior when no test failed in the last\n",
            "                        run or no information about the last failures was\n",
            "                        found in the cache\n",
            "\n",
            "reporting:\n",
            "  -v, --verbose         increase verbosity.\n",
            "  -q, --quiet           decrease verbosity.\n",
            "  --verbosity=VERBOSE   set verbosity\n",
            "  -r chars              show extra test summary info as specified by chars\n",
            "                        (f)ailed, (E)error, (s)skipped, (x)failed, (X)passed,\n",
            "                        (p)passed, (P)passed with output, (a)all except pP.\n",
            "                        Warnings are displayed at all times except when\n",
            "                        --disable-warnings is set\n",
            "  --disable-warnings, --disable-pytest-warnings\n",
            "                        disable warnings summary\n",
            "  -l, --showlocals      show locals in tracebacks (disabled by default).\n",
            "  --tb=style            traceback print mode (auto/long/short/line/native/no).\n",
            "  --show-capture={no,stdout,stderr,log,all}\n",
            "                        Controls how captured stdout/stderr/log is shown on\n",
            "                        failed tests. Default is 'all'.\n",
            "  --full-trace          don't cut any tracebacks (default is to cut).\n",
            "  --color=color         color terminal output (yes/no/auto).\n",
            "  --durations=N         show N slowest setup/test durations (N=0 for all).\n",
            "  --pastebin=mode       send failed|all info to bpaste.net pastebin service.\n",
            "  --junit-xml=path      create junit-xml style report file at given path.\n",
            "  --junit-prefix=str    prepend prefix to classnames in junit-xml output\n",
            "  --result-log=path     DEPRECATED path for machine-readable result log.\n",
            "\n",
            "collection:\n",
            "  --collect-only        only collect tests, don't execute them.\n",
            "  --pyargs              try to interpret all arguments as python packages.\n",
            "  --ignore=path         ignore path during collection (multi-allowed).\n",
            "  --deselect=nodeid_prefix\n",
            "                        deselect item during collection (multi-allowed).\n",
            "  --confcutdir=dir      only load conftest.py's relative to specified dir.\n",
            "  --noconftest          Don't load any conftest.py files.\n",
            "  --keep-duplicates     Keep duplicate tests.\n",
            "  --collect-in-virtualenv\n",
            "                        Don't ignore tests in a local virtualenv directory\n",
            "  --doctest-modules     run doctests in all .py modules\n",
            "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
            "                        choose another output format for diffs on doctest\n",
            "                        failure\n",
            "  --doctest-glob=pat    doctests file matching pattern, default: test*.txt\n",
            "  --doctest-ignore-import-errors\n",
            "                        ignore doctest ImportErrors\n",
            "  --doctest-continue-on-failure\n",
            "                        for a given doctest, continue to run after the first\n",
            "                        failure\n",
            "\n",
            "test session debugging and configuration:\n",
            "  --basetemp=dir        base temporary directory for this test run.\n",
            "  --version             display pytest lib version and import information.\n",
            "  -h, --help            show help message and configuration info\n",
            "  -p name               early-load given plugin (multi-allowed). To avoid\n",
            "                        loading of plugins, use the `no:` prefix, e.g.\n",
            "                        `no:doctest`.\n",
            "  --trace-config        trace considerations of conftest.py files.\n",
            "  --debug               store internal tracing debug information in\n",
            "                        'pytestdebug.log'.\n",
            "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
            "                        override ini option with \"option=value\" style, e.g.\n",
            "                        `-o xfail_strict=True -o cache_dir=cache`.\n",
            "  --assert=MODE         Control assertion debugging tools. 'plain' performs no\n",
            "                        assertion debugging. 'rewrite' (the default) rewrites\n",
            "                        assert statements in test modules on import to provide\n",
            "                        assert expression information.\n",
            "  --setup-only          only setup fixtures, do not execute tests.\n",
            "  --setup-show          show setup of fixtures while executing tests.\n",
            "  --setup-plan          show what fixtures and tests would be executed but\n",
            "                        don't execute anything.\n",
            "\n",
            "pytest-warnings:\n",
            "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
            "                        set which warnings to report, see -W option of python\n",
            "                        itself.\n",
            "\n",
            "logging:\n",
            "  --no-print-logs       disable printing caught logs on failed tests.\n",
            "  --log-level=LOG_LEVEL\n",
            "                        logging level used by the logging module\n",
            "  --log-format=LOG_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-date-format=LOG_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "  --log-cli-level=LOG_CLI_LEVEL\n",
            "                        cli logging level.\n",
            "  --log-cli-format=LOG_CLI_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "  --log-file=LOG_FILE   path to a file when logging will be written to.\n",
            "  --log-file-level=LOG_FILE_LEVEL\n",
            "                        log file logging level.\n",
            "  --log-file-format=LOG_FILE_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "\n",
            "custom options:\n",
            "  --myopt               some bool option\n",
            "  --pet=PET             pet: cat or dog\n",
            "\n",
            "\n",
            "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:\n",
            "\n",
            "  markers (linelist)       markers for test functions\n",
            "  empty_parameter_set_mark (string) default marker for empty parametersets\n",
            "  norecursedirs (args)     directory patterns to avoid for recursion\n",
            "  testpaths (args)         directories to search for tests when no files or dire\n",
            "  console_output_style (string) console output: classic or with additional progr\n",
            "  usefixtures (args)       list of default fixtures to be used with this project\n",
            "  python_files (args)      glob-style file patterns for Python test module disco\n",
            "  python_classes (args)    prefixes or glob names for Python test class discover\n",
            "  python_functions (args)  prefixes or glob names for Python test function and m\n",
            "  xfail_strict (bool)      default for the strict parameter of xfail markers whe\n",
            "  junit_suite_name (string) Test suite name for JUnit report\n",
            "  junit_logging (string)   Write captured log messages to JUnit report: one of n\n",
            "  doctest_optionflags (args) option flags for doctests\n",
            "  doctest_encoding (string) encoding used for doctest files\n",
            "  cache_dir (string)       cache directory path.\n",
            "  filterwarnings (linelist) Each line specifies a pattern for warnings.filterwar\n",
            "  log_print (bool)         default value for --no-print-logs\n",
            "  log_level (string)       default value for --log-level\n",
            "  log_format (string)      default value for --log-format\n",
            "  log_date_format (string) default value for --log-date-format\n",
            "  log_cli (bool)           enable log display during test run (also known as \"li\n",
            "  log_cli_level (string)   default value for --log-cli-level\n",
            "  log_cli_format (string)  default value for --log-cli-format\n",
            "  log_cli_date_format (string) default value for --log-cli-date-format\n",
            "  log_file (string)        default value for --log-file\n",
            "  log_file_level (string)  default value for --log-file-level\n",
            "  log_file_format (string) default value for --log-file-format\n",
            "  log_file_date_format (string) default value for --log-file-date-format\n",
            "  addopts (args)           extra command line options\n",
            "  minversion (string)      minimally required pytest version\n",
            "\n",
            "environment variables:\n",
            "  PYTEST_ADDOPTS           extra command line options\n",
            "  PYTEST_PLUGINS           comma-separated plugins to load during startup\n",
            "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals\n",
            "\n",
            "\n",
            "to see available markers type: pytest --markers\n",
            "to see available fixtures type: pytest --fixtures\n",
            "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH3UhSLwXckP",
        "outputId": "0098a366-665e-490d-b5ae-21bcb6c0c0f9"
      },
      "source": [
        "%%writefile test_config.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "\n",
        "def test_option(pytestconfig):\n",
        "  print('your pet is', pytestconfig.getoption('pet'))\n",
        "  print('your myopt is', pytestconfig.getoption('myopt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_config.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJl1o0LMX_xy",
        "outputId": "981b0497-272e-47cb-fa21-5e330d3d8e49"
      },
      "source": [
        "!pytest test_config.py --pet dog --myopt -s -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_config.py::test_option ('your pet is', 'dog')\n",
            "('your myopt is', True)\n",
            "\u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj6EL6KRYDL8",
        "outputId": "ae1e29bf-5400-4353-874f-8706d457d73c"
      },
      "source": [
        "!pytest test_config.py  -s -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_config.py::test_option ('your pet is', 'cat')\n",
            "('your myopt is', False)\n",
            "\u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6V4YTodYR5R",
        "outputId": "f17178db-6752-481b-d4fb-4fa797c07db7"
      },
      "source": [
        "%%writefile test_config2.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture()\n",
        "def pet(pytestconfig):\n",
        "  return pytestconfig.option.pet\n",
        "\n",
        "@pytest.fixture()\n",
        "def myopt(pytestconfig):\n",
        "  return pytestconfig.option.myopt\n",
        "\n",
        "def test_fixtures_for_options(pet, myopt):\n",
        "  print('my pet is', pet)\n",
        "  print('my opt is ', myopt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_config2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_4Q5gZEY1-p",
        "outputId": "9aa4bd0a-bb7b-440f-ead2-550036787f32"
      },
      "source": [
        "!pytest test_config2.py -s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_config2.py ('my pet is', 'cat')\n",
            "('my opt is ', False)\n",
            ".\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDyGGHaY3TQ",
        "outputId": "79b8b61a-089a-41ee-fda0-1bf5c751a0df"
      },
      "source": [
        "%%writefile test_config3.py \n",
        "\n",
        "def test_pytestconfig(pytestconfig):\n",
        "  print('args            :', pytestconfig.args)\n",
        "  print('inifile         :', pytestconfig.inifile)\n",
        "  print('invocation_dir  :', pytestconfig.invocation_dir)\n",
        "  print('rootdir         :', pytestconfig.rootdir)\n",
        "  print('-k EXPRESSION   :', pytestconfig.getoption('keyword'))\n",
        "  print('-v, --verbose   :', pytestconfig.getoption('verbose'))\n",
        "  print('-l, --showlocals:', pytestconfig.getoption('showlocals'))\n",
        "  print('--tb=style      :', pytestconfig.getoption('tbstyle'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_config3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvfYJjmbZIMF",
        "outputId": "c0dbe9bb-0a39-4290-e6dd-2f747d13ed25"
      },
      "source": [
        "!pytest test_config3.py -v -s -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_config3.py::test_pytestconfig ('args            :', ['test_config3.py'])\n",
            "('inifile         :', None)\n",
            "('invocation_dir  :', local('/content'))\n",
            "('rootdir         :', local('/content'))\n",
            "('-k EXPRESSION   :', '')\n",
            "('-v, --verbose   :', 1)\n",
            "('-l, --showlocals:', True)\n",
            "('--tb=style      :', 'auto')\n",
            "\u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfQXTtpga6v-"
      },
      "source": [
        "# Cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvuJ2x9QZMKv",
        "outputId": "cad125dc-f5ab-45ac-fbb2-deb4978b1847"
      },
      "source": [
        "!pytest --help | grep 'last-failed\\|failed-first\\|cache'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  --lf, --last-failed   rerun only the tests that failed at the last run (or\n",
            "  --ff, --failed-first  run all tests but run the last failures first. This\n",
            "  --cache-show          show cache contents, don't perform collection or tests\n",
            "  --cache-clear         remove all cache contents at start of test run.\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        found in the cache\n",
            "                        `-o xfail_strict=True -o cache_dir=cache`.\n",
            "  cache_dir (string)       cache directory path.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xutvdOadbaaY",
        "outputId": "602b3fe7-75c4-4537-a2ad-dba89d2044d1"
      },
      "source": [
        "%%writefile test_pass_fail.py\n",
        "\n",
        "def test_that_passes():\n",
        "  assert 1 == 1\n",
        "\n",
        "def test_that_fails():\n",
        "  assert 1 == 2\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_pass_fail.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUizuV6OcC8U",
        "outputId": "8be222d0-d9ab-4db9-a1a3-5976059168a8"
      },
      "source": [
        "!pytest test_pass_fail.py -v --tb=no"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_pass_fail.py::test_that_passes \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 50%]\u001b[0m\n",
            "test_pass_fail.py::test_that_fails \u001b[31mFAILED\u001b[0m\u001b[36m                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m====================== 1 failed, 1 passed in 0.03 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrJdEHiciZT"
      },
      "source": [
        "The tests are cached so that when \"ff\" (failed first) is called, it remembers which one failed first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMgAcCZfcFxm",
        "outputId": "9ef4bb7a-f60e-42ba-8e0a-f60a30226484"
      },
      "source": [
        "!pytest test_pass_fail.py -v --tb=no --ff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "run-last-failure: rerun previous 1 failure first\n",
            "\n",
            "test_pass_fail.py::test_that_fails \u001b[31mFAILED\u001b[0m\u001b[36m                                [ 50%]\u001b[0m\n",
            "test_pass_fail.py::test_that_passes \u001b[32mPASSED\u001b[0m\u001b[36m                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m====================== 1 failed, 1 passed in 0.02 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttC-QdNGcfzr",
        "outputId": "a0f034c5-fac8-42fc-f0fd-5fbdc92a187e"
      },
      "source": [
        "!pytest test_pass_fail.py -v --tb=no --last-failed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items / 1 deselected                                               \u001b[0m\n",
            "run-last-failure: rerun previous 1 failure\n",
            "\n",
            "test_pass_fail.py::test_that_fails \u001b[31mFAILED\u001b[0m\u001b[36m                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m==================== 1 failed, 1 deselected in 0.02 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO4LatlFctv_",
        "outputId": "6d3489e1-6e88-4a3d-d185-9e65c45bf8c3"
      },
      "source": [
        "%%writefile test_some_failures.py\n",
        "\n",
        "import pytest\n",
        "from pytest import approx\n",
        "\n",
        "test_data = [(1.01, 2.01, 3.02),\n",
        "             (1e25, 1e23, 1.1e25),\n",
        "             (1.23, 3.21, 4.44),\n",
        "             (0.1, 0.2, 0.3),\n",
        "             (1e25, 1e24, 1.1e25)]\n",
        "\n",
        "@pytest.mark.parametrize('x, y, expected', test_data)\n",
        "def test_a(x, y, expected):\n",
        "  sum_ = x + y\n",
        "  assert sum_ == approx(expected)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_some_failures.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XErV0yHwdTwZ",
        "outputId": "88131caa-a008-460f-d2e9-916c9dd916d7"
      },
      "source": [
        "!pytest test_some_failures.py -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".F...\u001b[36m                                                                    [100%]\u001b[0m\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________ test_a[1e+25-1e+23-1.1e+25] __________________________\u001b[0m\n",
            "\n",
            "x = 1e+25, y = 1e+23, expected = 1.1e+25\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize('x, y, expected', test_data)\u001b[0m\n",
            "\u001b[1m    def test_a(x, y, expected):\u001b[0m\n",
            "\u001b[1m      sum_ = x + y\u001b[0m\n",
            "\u001b[1m>     assert sum_ == approx(expected)\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 1.01e+25 == 1.1e+25 +- 1.1e+19\u001b[0m\n",
            "\u001b[1m\u001b[31mE      +  where 1.1e+25 +- 1.1e+19 = approx(1.1e+25)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_some_failures.py\u001b[0m:14: AssertionError\n",
            "\u001b[1m\u001b[31m1 failed, 4 passed in 0.04 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeHL5L-qdWv9",
        "outputId": "4fefa019-ab04-4e63-f7d3-6d13d25fcd3c"
      },
      "source": [
        "!pytest test_some_failures.py --lf -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items / 4 deselected                                               \u001b[0m\n",
            "run-last-failure: rerun previous 1 failure\n",
            "\n",
            "test_some_failures.py F\u001b[36m                                                  [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________ test_a[1e+25-1e+23-1.1e+25] __________________________\u001b[0m\n",
            "\n",
            "x = 1e+25, y = 1e+23, expected = 1.1e+25\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize('x, y, expected', test_data)\u001b[0m\n",
            "\u001b[1m    def test_a(x, y, expected):\u001b[0m\n",
            "\u001b[1m      sum_ = x + y\u001b[0m\n",
            "\u001b[1m>     assert sum_ == approx(expected)\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 1.01e+25 == 1.1e+25 +- 1.1e+19\u001b[0m\n",
            "\u001b[1m\u001b[31mE      +  where 1.1e+25 +- 1.1e+19 = approx(1.1e+25)\u001b[0m\n",
            "\n",
            "expected   = 1.1e+25\n",
            "sum_       = 1.01e+25\n",
            "x          = 1e+25\n",
            "y          = 1e+23\n",
            "\n",
            "\u001b[1m\u001b[31mtest_some_failures.py\u001b[0m:14: AssertionError\n",
            "\u001b[1m\u001b[31m==================== 1 failed, 4 deselected in 0.02 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsQQWlIVdqq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb392bed-7f49-4feb-c793-54b56a13afed"
      },
      "source": [
        "%%writefile test_slower.py\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def check_duration(request, cache):\n",
        "  key = 'duration/' + request.node.nodeid.replace(':', '_')\n",
        "  start_time = datetime.datetime.now()\n",
        "  yield\n",
        "  stop_time = datetime.datetime.now()\n",
        "  this_duration = (stop_time - start_time).total_seconds()\n",
        "  last_duration = cache.get(key, None)\n",
        "\n",
        "  cache.set(key, this_duration)\n",
        "  if last_duration is not None:\n",
        "    errorstring = \"test duration over 2x last duration\"\n",
        "    assert this_duration <= last_duration * 2, errorstring\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('i', range(5))\n",
        "def test_slow_stuff(i):\n",
        "  time.sleep(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_slower.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUUVPKq3eSSX",
        "outputId": "8850ebfa-b396-4a89-d32a-3516d557aefa"
      },
      "source": [
        "!pytest test_slower.py -q --tb=line"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....\u001b[36m                                                                    [100%]\u001b[0m\n",
            "\u001b[32m\u001b[1m5 passed in 1.52 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABWJLF8reUUN",
        "outputId": "ac4792ac-67ba-4359-edc4-098a19d497a7"
      },
      "source": [
        "!pytest -q --cache-show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cachedir: /content/.pytest_cache\n",
            "--------------------------------- cache values ---------------------------------\n",
            "cache/lastfailed contains:\n",
            "  {u'test_pass_fail.py::test_that_fails': True,\n",
            "   u'test_some_failures.py::test_a[1e+25-1e+23-1.1e+25]': True,\n",
            "   u'test_tmpdir.py::test_tmpdir': True}\n",
            "cache/nodeids contains:\n",
            "  [u'test_slower.py::test_slow_stuff[0]',\n",
            "   u'test_slower.py::test_slow_stuff[1]',\n",
            "   u'test_slower.py::test_slow_stuff[2]',\n",
            "   u'test_slower.py::test_slow_stuff[3]',\n",
            "   u'test_slower.py::test_slow_stuff[4]']\n",
            "duration/test_slower.py__test_slow_stuff[0] contains:\n",
            "  0.113925\n",
            "duration/test_slower.py__test_slow_stuff[1] contains:\n",
            "  0.168398\n",
            "duration/test_slower.py__test_slow_stuff[2] contains:\n",
            "  0.060191\n",
            "duration/test_slower.py__test_slow_stuff[3] contains:\n",
            "  0.646472\n",
            "duration/test_slower.py__test_slow_stuff[4] contains:\n",
            "  0.511017\n",
            "\n",
            "\u001b[1m\u001b[33mno tests ran in 0.00 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wqq_oz5e58y",
        "outputId": "f4adbd4a-9088-4098-ff19-bc8f25b6a9ae"
      },
      "source": [
        "%%writefile test_slower2.py\n",
        "from collections import namedtuple\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "\n",
        "import pytest\n",
        "\n",
        "Duration = namedtuple('Duration', ['current', 'last'])\n",
        "\n",
        "\n",
        "@pytest.fixture(scope='session')\n",
        "def duration_cache(request):\n",
        "  key = 'duration/testdurations'\n",
        "  d = Duration({}, request.config.cache.get(key, {}))\n",
        "  yield d\n",
        "  request.config.cache.set(key, d.current)\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def check_duration(request, duration_cache):\n",
        "  d = duration_cache\n",
        "  nodeid = request.node.nodeid\n",
        "  start_time = datetime.datetime.now()\n",
        "  yield\n",
        "  stop_time = datetime.datetime.now()\n",
        "  duration = (stop_time - start_time).total_seconds()\n",
        "  d.current[nodeid] = duration\n",
        "\n",
        "  if d.last.get(nodeid, None) is not None:\n",
        "    errorstring = \"test duration over 2x last duration\"\n",
        "    assert duration <= d.last[nodeid]* 2, errorstring\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('i', range(5))\n",
        "def test_slow_stuff(i):\n",
        "  time.sleep(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_slower2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOKsVlXng7dd",
        "outputId": "1219df8f-f229-434b-ac1e-c8260fbe54fe"
      },
      "source": [
        "!pytest --cache-clear test_slower2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_slower2.py .....\u001b[36m                                                    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 5 passed in 1.55 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9BejLRbgQTP",
        "outputId": "e3e0d3f0-ac25-4da9-86c0-0b9ad1d7a4ee"
      },
      "source": [
        "!pytest test_slower2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_slower2.py .E...E.E\u001b[36m                                                 [100%]\u001b[0m\n",
            "\n",
            "==================================== ERRORS ====================================\n",
            "___________________ ERROR at teardown of test_slow_stuff[0] ____________________\n",
            "\n",
            "request = <SubRequest 'check_duration' for <Function 'test_slow_stuff[0]'>>\n",
            "duration_cache = Duration(current={'test_slower2.py::test_slow_stuff[0]': 0.89221}, last={u'tes...st_slow_stuff[1]': 0.621141, u'test_slower2.py::test_slow_stuff[0]': 0.387305})\n",
            "\n",
            "\u001b[1m    @pytest.fixture(autouse=True)\u001b[0m\n",
            "\u001b[1m    def check_duration(request, duration_cache):\u001b[0m\n",
            "\u001b[1m      d = duration_cache\u001b[0m\n",
            "\u001b[1m      nodeid = request.node.nodeid\u001b[0m\n",
            "\u001b[1m      start_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      yield\u001b[0m\n",
            "\u001b[1m      stop_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      duration = (stop_time - start_time).total_seconds()\u001b[0m\n",
            "\u001b[1m      d.current[nodeid] = duration\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      if d.last.get(nodeid, None) is not None:\u001b[0m\n",
            "\u001b[1m        errorstring = \"test duration over 2x last duration\"\u001b[0m\n",
            "\u001b[1m>       assert duration <= d.last[nodeid]* 2, errorstring\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: test duration over 2x last duration\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 0.89221 <= (0.387305 * 2)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_slower2.py\u001b[0m:30: AssertionError\n",
            "___________________ ERROR at teardown of test_slow_stuff[3] ____________________\n",
            "\n",
            "request = <SubRequest 'check_duration' for <Function 'test_slow_stuff[3]'>>\n",
            "duration_cache = Duration(current={'test_slower2.py::test_slow_stuff[3]': 0.358413, 'test_slowe...st_slow_stuff[1]': 0.621141, u'test_slower2.py::test_slow_stuff[0]': 0.387305})\n",
            "\n",
            "\u001b[1m    @pytest.fixture(autouse=True)\u001b[0m\n",
            "\u001b[1m    def check_duration(request, duration_cache):\u001b[0m\n",
            "\u001b[1m      d = duration_cache\u001b[0m\n",
            "\u001b[1m      nodeid = request.node.nodeid\u001b[0m\n",
            "\u001b[1m      start_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      yield\u001b[0m\n",
            "\u001b[1m      stop_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      duration = (stop_time - start_time).total_seconds()\u001b[0m\n",
            "\u001b[1m      d.current[nodeid] = duration\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      if d.last.get(nodeid, None) is not None:\u001b[0m\n",
            "\u001b[1m        errorstring = \"test duration over 2x last duration\"\u001b[0m\n",
            "\u001b[1m>       assert duration <= d.last[nodeid]* 2, errorstring\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: test duration over 2x last duration\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 0.358413 <= (0.066201 * 2)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_slower2.py\u001b[0m:30: AssertionError\n",
            "___________________ ERROR at teardown of test_slow_stuff[4] ____________________\n",
            "\n",
            "request = <SubRequest 'check_duration' for <Function 'test_slow_stuff[4]'>>\n",
            "duration_cache = Duration(current={'test_slower2.py::test_slow_stuff[4]': 0.869384, 'test_slowe...st_slow_stuff[1]': 0.621141, u'test_slower2.py::test_slow_stuff[0]': 0.387305})\n",
            "\n",
            "\u001b[1m    @pytest.fixture(autouse=True)\u001b[0m\n",
            "\u001b[1m    def check_duration(request, duration_cache):\u001b[0m\n",
            "\u001b[1m      d = duration_cache\u001b[0m\n",
            "\u001b[1m      nodeid = request.node.nodeid\u001b[0m\n",
            "\u001b[1m      start_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      yield\u001b[0m\n",
            "\u001b[1m      stop_time = datetime.datetime.now()\u001b[0m\n",
            "\u001b[1m      duration = (stop_time - start_time).total_seconds()\u001b[0m\n",
            "\u001b[1m      d.current[nodeid] = duration\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      if d.last.get(nodeid, None) is not None:\u001b[0m\n",
            "\u001b[1m        errorstring = \"test duration over 2x last duration\"\u001b[0m\n",
            "\u001b[1m>       assert duration <= d.last[nodeid]* 2, errorstring\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: test duration over 2x last duration\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 0.869384 <= (0.028405 * 2)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_slower2.py\u001b[0m:30: AssertionError\n",
            "\u001b[1m\u001b[31m====================== 5 passed, 3 error in 3.11 seconds =======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv6u_3x3gRkF",
        "outputId": "a8768c5e-6915-4b7a-b13c-d4ae1e6524bf"
      },
      "source": [
        "!pytest --cache-show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "cachedir: /content/.pytest_cache\n",
            "--------------------------------- cache values ---------------------------------\n",
            "cache/lastfailed contains:\n",
            "  {u'test_slower2.py::test_slow_stuff[0]': True,\n",
            "   u'test_slower2.py::test_slow_stuff[3]': True,\n",
            "   u'test_slower2.py::test_slow_stuff[4]': True}\n",
            "cache/nodeids contains:\n",
            "  [u'test_slower2.py::test_slow_stuff[0]',\n",
            "   u'test_slower2.py::test_slow_stuff[1]',\n",
            "   u'test_slower2.py::test_slow_stuff[2]',\n",
            "   u'test_slower2.py::test_slow_stuff[3]',\n",
            "   u'test_slower2.py::test_slow_stuff[4]']\n",
            "duration/testdurations contains:\n",
            "  {u'test_slower2.py::test_slow_stuff[0]': 0.89221,\n",
            "   u'test_slower2.py::test_slow_stuff[1]': 0.131511,\n",
            "   u'test_slower2.py::test_slow_stuff[2]': 0.821144,\n",
            "   u'test_slower2.py::test_slow_stuff[3]': 0.358413,\n",
            "   u'test_slower2.py::test_slow_stuff[4]': 0.869384}\n",
            "\n",
            "\u001b[1m\u001b[33m========================= no tests ran in 0.00 seconds =========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVWUsBKhQS_"
      },
      "source": [
        "# Using capsys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqB80rshVDr"
      },
      "source": [
        "capsys allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgptDLTRg3DS",
        "outputId": "d66453c7-6f14-4a44-ba25-e935b5e580fd"
      },
      "source": [
        "%%writefile test_capsys.py\n",
        "\n",
        "def greeting(name):\n",
        "  print('Hi, %s'%name)\n",
        "\n",
        "def test_greeting(capsys):\n",
        "  greeting('Earthling')\n",
        "  out, err = capsys.readouterr()\n",
        "  assert out == 'Hi, Earthling\\n'\n",
        "  assert err == \"\"\n",
        "\n",
        "  greeting('Brian')\n",
        "  greeting('Ned')\n",
        "  out, err = capsys.readouterr()\n",
        "\n",
        "  assert out == 'Hi, Brian\\nHi, Ned\\n'\n",
        "  assert err == ''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_capsys.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5cRLb0Ih7FP",
        "outputId": "c548623d-feca-40cc-aef3-7940dc71420c"
      },
      "source": [
        "!pytest test_capsys.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_capsys.py::test_greeting \u001b[32mPASSED\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jQPo7iHh8kd",
        "outputId": "0cce6b77-815c-4083-b517-ecbab94fe2c8"
      },
      "source": [
        "%%writefile test_capsys2.py\n",
        "import sys\n",
        "\n",
        "def yikes(problem):\n",
        "  print('Yikes! %s'%problem, file=sys.stderr)\n",
        "\n",
        "\n",
        "def test_yikes(capsys):\n",
        "  yikes('Out of batterie!')\n",
        "  out, err = capsys.readouterr()\n",
        "\n",
        "  assert out == \"\"\n",
        "  assert 'Out of batteries!' in err"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_capsys2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCrGT1pEi3us",
        "outputId": "fc844b26-eb93-4ca2-aed1-49feb9d24b35"
      },
      "source": [
        "!python3 -m pytest test_capsys2.py -s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_capsys2.py F\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m__________________________________ test_yikes __________________________________\u001b[0m\n",
            "\n",
            "capsys = <_pytest.capture.CaptureFixture object at 0x7f9d9eb22dd0>\n",
            "\n",
            "\u001b[1m    def test_yikes(capsys):\u001b[0m\n",
            "\u001b[1m      yikes('Out of batterie!')\u001b[0m\n",
            "\u001b[1m      out, err = capsys.readouterr()\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m      assert out == \"\"\u001b[0m\n",
            "\u001b[1m>     assert 'Out of batteries!' in err\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'Out of batteries!' in 'Yikes! Out of batterie!\\n'\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_capsys2.py\u001b[0m:12: AssertionError\n",
            "\u001b[31m\u001b[1m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_bUjyh6lUf0"
      },
      "source": [
        "# Print without pytest capturing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zarKmgXi5Q2",
        "outputId": "e6a6736e-b75a-4066-bf47-ad5b802f6d9d"
      },
      "source": [
        "%%writefile test_capsys3.py\n",
        "def test_capsys_disabled(capsys):\n",
        "  with capsys.disabled():\n",
        "    print('\\nalways print this')\n",
        "  print('normal print, captured!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_capsys3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4qt_LQGj_og",
        "outputId": "4e69e8b9-ded5-4d96-d07d-6940c1f1ddf1"
      },
      "source": [
        "!pytest test_capsys3.py -s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_capsys3.py \n",
            "always print this\n",
            "normal print, captured!\n",
            ".\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op5fH6VjlFB7",
        "outputId": "eb761e68-47d7-4303-df69-dafc09e02629"
      },
      "source": [
        "!pytest test_capsys3.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_capsys3.py \n",
            "always print this\n",
            ".\u001b[36m                                                        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxmh0EKHldP1"
      },
      "source": [
        "# Using monkeypatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFdNyoNolfCY"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def read_cheese_preferences():\n",
        "  full_path = os.path.expanduser('~/.cheese.json')\n",
        "  with open(full_path) as f:\n",
        "    prefs = json.load(f)\n",
        "  return prefs\n",
        "\n",
        "def write_cheese_preferences(prefs):\n",
        "  full_path = os.path.expanduser('~/.cheese.json')\n",
        "  with open(full_path, 'w') as f:\n",
        "    json.dumps(prefs, f, indent=4)\n",
        "\n",
        "\n",
        "def write_default_cheese_preferences():\n",
        "  write_cheese_preferences(_default_prefs)\n",
        "\n",
        "_default_prefs = {\n",
        "    'slicing': ['manchego', 'sharp cheddar'],\n",
        "    'spreadable': ['Saint Andre', 'camembert'],\n",
        "    'salads': ['crumbled feta']\n",
        "}\n",
        "\n",
        "def test_def_prefs_change_home(tmpdir, monkeypatch):\n",
        "  fake_home_dir = tmpdir.mkdir('home')\n",
        "  monkeypatch.setattr(cheese.os.path, 'expanduser',\n",
        "                      lambda x: x.replace('~', str(fake_home_dir)))\n",
        "  \n",
        "  cheese.write_default_cheese_preferences()\n",
        "  defaults_before = copy.deepcopy(cheese._default_prefs)\n",
        "\n",
        "  # change the defaults\n",
        "  monkeypatch.setitem(cheese._default_prefs, 'slicing', ['provolone'])\n",
        "  monkeypatch.setitem(cheese._default_prefs, 'spreadable', ['brie'])\n",
        "  monkeypatch.setitem(cheese._default_prefs, 'salads', ['pepper jack'])\n",
        "  defaults_modified = cheese._default_prefs\n",
        "  \n",
        "  cheese.write_default_cheese_preferences()\n",
        "\n",
        "  actual = cheese.read_cheese_preferences()\n",
        "  assert defaults_modified == actual\n",
        "  assert defaults_modified != defaults_before"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf-IL5pw7iwg"
      },
      "source": [
        "syspath_prepend(path): Adds a new path to your list of module import directories. This can be used to replace a system-wide module or package with a fake version.\n",
        "\n",
        "\n",
        "chdir(path): change the current working directory. You could change to a temporary directory that holds whatever contents you need for a command-line script or something"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MieFojN-8QPS"
      },
      "source": [
        "monkeypatch fixtures also work in conjunction with unittest.mock to temporarily replace attributes with mock objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAGR-Y9v8dyb"
      },
      "source": [
        "# Using doctest_namespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCwgcplclOGS",
        "outputId": "6aaf568d-4ad7-42e1-db8f-54a884ba01b0"
      },
      "source": [
        "%%writefile my_math.py\n",
        "\"\"\"\n",
        "This file defines multiply(a, b) and divide(a, b).\n",
        "\n",
        ">>> import my_math as mm\n",
        "\n",
        "Here's how you use multiply:\n",
        "\n",
        ">>> mm.multiply(4, 3)\n",
        "12\n",
        "\n",
        ">>> mm.multiply('a', 3)\n",
        "'aaa'\n",
        "\n",
        "\n",
        ">>> mm.divide(10, 5)\n",
        "2.0\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def multiply(a, b):\n",
        "  \"\"\"\n",
        "  Returns a * b.\n",
        "\n",
        "  >>> mm.multiply(4, 3)\n",
        "  12\n",
        "\n",
        "  >>> mm.multiply('a', 3)\n",
        "  'aaa'\n",
        "  \"\"\"\n",
        "  return a*b\n",
        "\n",
        "def divide(a, b):\n",
        "  \"\"\"\n",
        "  Returns a / b\n",
        "\n",
        "  >>> mm.divide(10, 5)\n",
        "  2.0\n",
        "  \"\"\"\n",
        "  return a / b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting my_math.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_u0ErLzXtcs",
        "outputId": "0db84851-db33-4339-8ffc-fec4b6836704"
      },
      "source": [
        "!python3 -m pytest -v --doctest-modules --tb=short my_math.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "my_math.py::my_math \u001b[32mPASSED\u001b[0m\u001b[36m                                               [ 33%]\u001b[0m\n",
            "my_math.py::my_math.divide \u001b[31mFAILED\u001b[0m\u001b[36m                                        [ 66%]\u001b[0m\n",
            "my_math.py::my_math.multiply \u001b[31mFAILED\u001b[0m\u001b[36m                                      [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m___________________________ [doctest] my_math.divide ___________________________\u001b[0m\n",
            "033 \n",
            "034   Returns a / b\n",
            "035 \n",
            "036   >>> mm.divide(10, 5)\n",
            "UNEXPECTED EXCEPTION: NameError(\"name 'mm' is not defined\")\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/lib/python3.7/doctest.py\", line 1337, in __run\n",
            "    compileflags, 1), test.globs)\n",
            "\n",
            "  File \"<doctest my_math.divide[0]>\", line 1, in <module>\n",
            "\n",
            "NameError: name 'mm' is not defined\n",
            "\n",
            "\u001b[1m\u001b[31m/content/my_math.py\u001b[0m:36: UnexpectedException\n",
            "\u001b[31m\u001b[1m__________________________ [doctest] my_math.multiply __________________________\u001b[0m\n",
            "021 \n",
            "022   Returns a * b.\n",
            "023 \n",
            "024   >>> mm.multiply(4, 3)\n",
            "UNEXPECTED EXCEPTION: NameError(\"name 'mm' is not defined\")\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/lib/python3.7/doctest.py\", line 1337, in __run\n",
            "    compileflags, 1), test.globs)\n",
            "\n",
            "  File \"<doctest my_math.multiply[0]>\", line 1, in <module>\n",
            "\n",
            "NameError: name 'mm' is not defined\n",
            "\n",
            "\u001b[1m\u001b[31m/content/my_math.py\u001b[0m:24: UnexpectedException\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.02 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auj4pR3fXxat",
        "outputId": "a41cf7e2-437e-4288-ec1b-87ad3d4748ef"
      },
      "source": [
        "%%writefile conftest.py\n",
        "import pytest\n",
        "\n",
        "import my_math\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def add_mm(doctest_namespace):\n",
        "  doctest_namespace['mm'] = my_math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGhUNaJ4YkFx",
        "outputId": "b6d2bc1b-c175-4b83-af67-e7152dd2d3b8"
      },
      "source": [
        "!python3 -m pytest -v --doctest-modules --tb=short my_math.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.11, pytest-3.6.4, py-1.10.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "my_math.py::my_math \u001b[32mPASSED\u001b[0m\u001b[36m                                               [ 33%]\u001b[0m\n",
            "my_math.py::my_math.divide \u001b[32mPASSED\u001b[0m\u001b[36m                                        [ 66%]\u001b[0m\n",
            "my_math.py::my_math.multiply \u001b[32mPASSED\u001b[0m\u001b[36m                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 3 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFiD1iKEYtGa"
      },
      "source": [
        "# Using recwarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfFpxyKGYmgq",
        "outputId": "47aa4f2a-4ab8-472b-afb8-6ba17e1fc8ab"
      },
      "source": [
        "%%writefile test_warnings.py\n",
        "import warnings\n",
        "import pytest\n",
        "\n",
        "\n",
        "def bad_function():\n",
        "  warnings.warn('This is bad', DeprecationWarning)\n",
        "\n",
        "\n",
        "def test_bad_function(recwarn):\n",
        "  bad_function()\n",
        "  assert len(recwarn) == 1\n",
        "\n",
        "  w = recwarn.pop()\n",
        "  assert w.category == DeprecationWarning\n",
        "  assert str(w.message) == 'This is bad'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_warnings.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "misQbGR6ZRtj",
        "outputId": "9312cbde-9564-4591-e9bf-9db1fbcb1e47"
      },
      "source": [
        "!pytest test_warnings.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_warnings.py .\u001b[36m                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMwMKgHjZkGJ",
        "outputId": "f4ec9ca1-70e7-4f1f-aa2a-80c99e764227"
      },
      "source": [
        "%%writefile test_warnings2.py\n",
        "import warnings\n",
        "import pytest\n",
        "\n",
        "\n",
        "def bad_function():\n",
        "  warnings.warn('This is bad', DeprecationWarning)\n",
        "\n",
        "def test_bad_function2():\n",
        "  with pytest.warns(None) as warning_list:\n",
        "    bad_function()\n",
        "\n",
        "  assert len(warning_list) == 1\n",
        "\n",
        "  w = warning_list.pop()\n",
        "  assert w.category == DeprecationWarning\n",
        "  assert str(w.message) == 'This is bad'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_warnings2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ju2Fi7vaC4f",
        "outputId": "35e0faf9-2741-4774-9e5d-d02e67a23287"
      },
      "source": [
        "!pytest test_warnings2.py -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_warnings2.py::test_bad_function2 \u001b[32mPASSED\u001b[0m\u001b[36m                             [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}